{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Linear_Regression.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vguMN1fkROzC"
      },
      "source": [
        "# Linear Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P1d5PJQ7R-VE"
      },
      "source": [
        "Linear regression is a linear model, e.g. a model that assumes a linear relationship between the input variables (x) and the single output variable (y)\n",
        "\n",
        "Example:  \n",
        "\n",
        "*   If we want age, weight, sex and other parameters of people and we want to predict the height of person, this is linear regression problem.\n",
        "*   Predicting house price using it's features(area, location, age...) is also a linear regression problem.\n",
        "\n",
        "Firstly we discuss linear regression with one feature(target is dependentonly on one variable)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IDX503Olgdny"
      },
      "source": [
        "## Linear Regression with One Variable:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R3mpzpiBWl_7"
      },
      "source": [
        "![](https://46gyn61z4i0t1u1pnq2bbk2e-wpengine.netdna-ssl.com/wp-content/uploads/2018/04/Example-of-simple-regression.png)\n",
        "\n",
        "On this image we see information about a companys sales and advertising over years. We can mak a scatter plot for this kind of data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pErIdhOGUoXT",
        "outputId": "05acdf77-1a3a-4a1c-e3fd-1239cef0b8d6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "X = [23, 26, 30, 34, 43, 48, 52, 57, 58]  # advertising\n",
        "y = [651, 762, 856, 1063, 1190, 1298, 1421, 1440, 1518]  # sales\n",
        "\n",
        "plt.scatter(X, y)\n",
        "plt.xlabel('advertising')\n",
        "plt.ylabel('sales')\n",
        "plt.show()"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3TXUUiYtW5sD"
      },
      "source": [
        "We need to create a model that predicts sales using advertising, so input for our model will be advertismenet column and it should predict sales column. The most common way to solve this problem is to use one variable linear regression.\n",
        "\n",
        "We should have this function: $f(x) = w_0 + w_{1}x$ where $w_1$ indicates slope of line and $w_0$ is a free parameter. Usually we call weights to these parameters. If $w_1 = 0$, then values of $y$-are not dependent on $X$; If $w_1 > 0$, $y$ increases when $X$ is increasing, if $w_1 < 0$ our function is decreasing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e2suOEcNibsM"
      },
      "source": [
        "We should assign some initial value to our weights(parameters) and we should correct them several times until we reduce error_cost to a minimum. Usually, in case of linear regression, initial values are zeros. We need this values and  $L$ variable:\n",
        "\n",
        "*   $w_0^{0} = 0$ and $w_1^{0} = 0$.  \n",
        "*   Using $L$ variable we control how much is delta(change) between $w = (w_0, w_1)$ cofficients while going from one epoch to another. The name of this variable is Learning Rate, it must have positive value.\n",
        "\n",
        "So our model has a line according to this function: $f(x) = w_0^{0} + w_1^{0}x$ , on each step it generates new parameters $w^1 = (w_0^{1}, w_1^{1})$ before converging and this steps is called epochs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p_5FPozTir7i"
      },
      "source": [
        "As we already said, our main goal is to reduce value of error. Error function is dependent on variables $w_0$ and $w_1$ and its name is cost/loss function. Very often people use mean squared error to define loss function:\n",
        "$$J(w) = J(w_0, w_1) = \\frac{1}{2n} \\sum_{i=1}^{n}(y_i^* - y_i)^2 = \\frac{1}{2n} \\sum_{i=1}^{n}(w_0 + w_1x_i - y_i)^2$$  \n",
        "where $n$ is number of rows, $y_i^\\ast$ - predicted ourput by model in the $x_i$ point ( $y_i^\\ast = f(x_i)$ ).\n",
        "\n",
        "* Why people use Squared Error? - First of all it always has nonnegative values and doesn't try to reduce cost function in the way of getting bigger negative value. Also it doesn't have local extremas, it has only global minimum and is differentiable in any point. \n",
        "  \n",
        "* Why people use MEAN Squared Error? - When we calculate squared error, it increases automatically if we increase row number so we need to take an average.\n",
        "\n",
        "* Why people use 1/2 coefficient? - Differential gives us extra '2' and we just need reduction of it so we use 1/2. In fact it is not necessary, it's just more beautiful(we need differentiate to minimize the function)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gNMVUvivTqTc"
      },
      "source": [
        "Cost function of linear regression is like this:  \n",
        "![](https://i.ytimg.com/vi/riplXsNf_zs/maxresdefault.jpg)  \n",
        "axes on down represent $w_0, w_1$ parameters, upper axis represents value of cost function. Point $A$ is starting point - $(w_0^0, w_1^0, J(w_0^0, w_1^0))$ and point $B$ is our target(minimum of the function). To get from point $A$ to point $B$ we should use __gradient descent__ algorithm. Gradient is the same as slope/differential(in our case it will be partial differentials). Also sometimes vector of partial differential of function is called as gradient: $\\nabla f(x, y) = (\\frac{\\partial f(x, y)}{\\partial x}, \\frac{\\partial f(x, y)}{\\partial y}) $.  \n",
        "As we use gradient descent to reduce cost function, it is called optimizer(there are other optimizers too). It reduces the value of cost function step-by-step and the size of this step is denoted by learning rate. If our step is too big, we may miss the minimum point and if it is too small than we may need lots of time to reach a minimum and learning rate controls exactly the step size.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7MilnsOkvFqW"
      },
      "source": [
        "Using gradient descent we change coefficients $w^j = (w_0^{j}, w_1^{j})$  ($j$ is number of epoch ) according to this rule:\n",
        "\n",
        "\n",
        "* Let's look at our cost function:\n",
        "\n",
        "$$ J(w^j) = J(w_0^j, w_1^j) = \\frac{1}{2n} \\sum_{i=1}^{n}(w_0^j + w_1^jx_i - y_i)^2 $$\n",
        "\n",
        "* We should evaluate partial derivatives of $J(w^j)$ so the slope of the function along each variable:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial J(w^j) }{\\partial w_0^{j}} = \\frac{1}{n} \\sum_{i=1}^{n}(y_i^{\\ast} - y_i)\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\frac{\\partial J(w^j) }{\\partial w_1^{j}} = \\frac{1}{n} \\sum_{i=1}^{n}(y_i^{\\ast} - y_i)x_i\n",
        "$$\n",
        "\n",
        "*   Then we use that values to calculate new values for our variables $(w_0, w_1)$ \n",
        "\n",
        "$$\n",
        "w_0^{j+1} = w_0^{j} - L \\frac{\\partial J(w^j) }{\\partial w_0^{j}} = \n",
        "w_0^{j} - L \\frac{1}{n} \\sum_{i=1}^{n}(y_i^{\\ast} - y_i)\n",
        "$$\n",
        "\n",
        "$$\n",
        "w_1^{j+1} = w_1^{j} - L \\frac{\\partial J(w^j) }{\\partial w_1^{j}} =\n",
        "w_1^{j} - L \\frac{1}{n} \\sum_{i=1}^{n}(y_i^{\\ast} - y_i)x_i\n",
        "$$\n",
        "\n",
        "This means that our variables travel along the direction of decrease. As we already saw, in case of linear regression we always have global minimum so if we choose good learning rate, we will always reach the global minimum. Usually we try L=0.01 or L=0.001 but we should try some different values too."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1sYhhtHb0Au8"
      },
      "source": [
        "It's also important to make sure that $X$ is in range of $[0: 1]$ because if it is in some big range than gradient descent will be much slower."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wGLgKjeEH3aP"
      },
      "source": [
        "def gradient_descent(X, y, w0_init=0, w1_init=0, L=1e-2, tol=1e-3, max_iters=10000):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "          X : One dimensional numpy array (n, )\n",
        "          y : One dimensional numpy array (n, )\n",
        "\n",
        "    w0_init : initial value of w0 (default 0)\n",
        "    w1_init : initial value of w1 (default 0)\n",
        "    L : learning rate (default 1e-2)\n",
        "    tol : tolerance (default 1e-3)\n",
        "    max_iters: maximum number of epochs\n",
        "    \n",
        "    Returns:\n",
        "          tuple with two floats : w1, w0\n",
        "    \"\"\"\n",
        "    w0, w1 = w0_init, w1_init  # initialize of weights\n",
        "\n",
        "    n = len(X) # num rows\n",
        "\n",
        "    # linear regression\n",
        "    for i in range(max_iters): \n",
        "        y_pred = w1*X + w0  # predicted output\n",
        "        err = y_pred - y   # error\n",
        "        D_w1 = (1/n) * sum(X*err) # derivative along w1\n",
        "        D_w0 = (1/n) * sum(err)  # derivative along w0\n",
        "        if abs(L*D_w1) <= tol and abs(L*D_w0) <= tol:  # If weights are almost the same, let's stop here\n",
        "            break\n",
        "        w1 = w1 - L * D_w1  # update w1\n",
        "        w0 = w0 - L * D_w0  # update w1 \n",
        "    return (w1, w0)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iO8xhWVU2BVU",
        "outputId": "21d457f6-b8be-4de3-e409-944789730b76",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "X, y = np.array(X), np.array(y)  # convert to numpy array\n",
        "X = (X - X.min())/(X.max() - X.min())  # scaling X in range [0: 1] \n",
        "w1, w0 = gradient_descent(X, y) \n",
        "w1, w0"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(818.8000844493249, 706.9841348102336)"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hsKB2BF2BQb1"
      },
      "source": [
        "Let's make some visualization:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qPBc3RSswr-G",
        "outputId": "d3c6cd5e-ecf6-432c-c34e-39f6070a333a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        }
      },
      "source": [
        "plt.scatter(X, y)\n",
        "plt.plot([0, 1], [w0, w1 + w0], c='orange') \n",
        "plt.show()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0UWwqGSjVquC"
      },
      "source": [
        "## Multivariable Linear Regression\n",
        "\n",
        "We have more than one($m$) independent variable/feature, so we will have $m+1$ weights. Now $X$ looks like this:\n",
        "$$\n",
        "X_{n,m}= \n",
        "\\begin{pmatrix}\n",
        "  x_{1,1} & x_{1,2} & \\cdots & x_{1,m} \\\\\n",
        "  x_{2,1} & x_{2,2} & \\cdots & x_{2,m} \\\\\n",
        "  \\vdots  & \\vdots  & \\ddots & \\vdots  \\\\\n",
        "  x_{n,1} & x_{n,2} & \\cdots & x_{n,m} \n",
        "\\end{pmatrix}\n",
        "$$\n",
        "\n",
        "and our line equation is like this:\n",
        "$$f(x) = w_0 + w_1x_1 + \\cdots + w_mx_m$$ \n",
        "\n",
        "Initial values are still zeros\n",
        "\n",
        "*   $w_0^{0} = 0$, $w_1^{0} = 0$ ... $w_m^{0} = 0$. \n",
        "\n",
        " $f(x) = w_0^{0} + w_1^{0}x_1 + \\cdots + w_m^{0}x_m$ this is our line, we should update these parameters: $(w_0^{1}, w_1^{1}, \\cdots, w_m^1)$ while they don't converge\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2KaI6VhPdzsB"
      },
      "source": [
        "Process of updating weights is the same as one variable linear regression, we should update variables: $w^j = (w_0^{j}, w_1^{j}, \\cdots, w_m^{j})$ ($j$ is epoch number):\n",
        "\n",
        "* Let's look at cost function\n",
        "\n",
        "$$ J(w^j) = \\frac{1}{2n} \\sum_{i=1}^{n}(y_i^\\ast - y_i)^2 = \\frac{1}{2n} \\sum_{i=1}^{n}(w_0^j + w_1^jx_{i, 1} + \\cdots + w_m^jx_{i, m} - y_i)^2 $$\n",
        "\n",
        "* We should calculate each partial derivative of function $J(w^j)$ along each variable $w_k^{j}$ (so we should evaluate slope for each variable):\n",
        "$$\n",
        "\\frac{\\partial J(w^j) }{\\partial w_0^{j}} = \\frac{1}{n} \\sum_{i=1}^{n}(y_i^{\\ast} - y_i)\n",
        "$$\n",
        "for any other $w_k^{j}$:\n",
        "$$\n",
        "\\frac{\\partial J(w^j) }{\\partial w_k^{j}} = \\frac{1}{n} \\sum_{i=1}^{n}(y_i^{\\ast} - y_i)x_{i,k}\n",
        "$$\n",
        "\n",
        "* Then we calculate new values for $w$ : \n",
        "$$\n",
        "w_0^{j+1} = w_0^{j} - L \\frac{\\partial J(w^j) }{\\partial w_0^{j}} = \n",
        "w_0^{j} - L \\frac{1}{n} \\sum_{i=1}^{n}(y_i^{\\ast} - y_i)\n",
        "$$\n",
        "$$\n",
        "w_k^{j+1} = w_k^{j} - L \\frac{\\partial J(w^j) }{\\partial w_k^{j}} =\n",
        "w_k^{j} - L \\frac{1}{n} \\sum_{i=1}^{n}(y_i^{\\ast} - y_i)x_{i, k}\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3RSae8rfnV-M"
      },
      "source": [
        "All features should be nearly in the same range, this makes gradient descent much faster. We can use feature scaling or standartization for that."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Isfqsj4BokPY"
      },
      "source": [
        "## Vectorization\n",
        "\n",
        "Gradient descent is pretty 'expensive' algorithm(it needs much time and computational resources as we calculate partial derivatives in each step). To fix this we can use vectorization method.\n",
        "We should write features as matrix and add column '1' in matrix $X$. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lpXF8XMEqLsg"
      },
      "source": [
        "Our matrix looks like this:  \n",
        "$$ \n",
        "X_{n,m+1}= \n",
        "\\begin{pmatrix}\n",
        "  x_{1,0} & x_{1,1} & \\cdots & x_{1,m} \\\\\n",
        "  x_{2,0} & x_{2,1} & \\cdots & x_{2,m} \\\\\n",
        "  \\vdots  & \\vdots  & \\ddots & \\vdots  \\\\\n",
        "  x_{n,0} & x_{n,1} & \\cdots & x_{n,m} \n",
        "\\end{pmatrix}\n",
        "$$\n",
        "  \n",
        "Each $x_{i, 0}=1$, so we added one more feature that is always '1'. now we can write partial derivative of cost function like this:\n",
        "$$\n",
        "\\frac{\\partial J(w^j) }{\\partial w_k^{j}} = \\frac{1}{n} \\sum_{i=1}^{n}(y_i^{\\ast} - y_i)x_{i,k}\n",
        "$$  \n",
        "and also predicted value: \n",
        "$$y_i^\\ast = x_i^Tw^j$$\n",
        "So thev ector of partial derivative of cost function will be like this:\n",
        "  \n",
        "$$\n",
        "\\frac{\\partial J(w^j) }{\\partial w^{j}} = \n",
        "\\frac{1}{n}\n",
        "\\begin{pmatrix}\n",
        "  \\sum_{i=1}^{n}(x_i^Tw^j - y_i)x_{i,0}\\\\\n",
        "  \\sum_{i=1}^{n}(x_i^Tw^j - y_i)x_{i,1}\\\\\n",
        "  \\vdots\\\\\n",
        "  \\sum_{i=1}^{n}(x_i^Tw^j - y_i)x_{i,m}\\\\\n",
        "\\end{pmatrix}\n",
        "$$  \n",
        "\n",
        "and this is the same as: $\\frac{1}{n}X^T(Xw^j-y)$. It is pretty easy to notice if we look compunents one by one. \n",
        "$$ \n",
        "X_{m+1,n}^T= \n",
        "\\begin{pmatrix}\n",
        "  x_{1,0} & x_{2,0} & \\cdots & x_{n,0} \\\\\n",
        "  x_{1,1} & x_{2,1} & \\cdots & x_{n,1} \\\\\n",
        "  \\vdots  & \\vdots  & \\ddots & \\vdots  \\\\\n",
        "  x_{1,m} & x_{2,m} & \\cdots & x_{n,m} \n",
        "\\end{pmatrix}\n",
        "$$\n",
        "\n",
        "and \n",
        "\n",
        "$$ \n",
        "Xw^j-y= \n",
        "\\begin{pmatrix}\n",
        "  x_0^Tw^j - y_0 \\\\\n",
        "  x_1^Tw^j - y_1 \\\\\n",
        "  \\vdots \\\\\n",
        "  x_n^Tw^j - y_n \n",
        "\\end{pmatrix}\n",
        "$$\n",
        "\n",
        "If we take product of them on  ($\\frac{1}{n}$) we'll get exactly $\\frac{\\partial J(w^j) }{\\partial w^{j}}$.\n",
        "So we can update weights with just simple substraction of mvectors:\n",
        "$$ w^{j+1} = w^j - L \\frac{1}{n}X^T(Xw^j-y) $$ \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k7qDM4s9Ew1P"
      },
      "source": [
        "def vectorized_gradient_descent(X, y, w_init, L=1e-2, tol=1e-3, max_iters=10000):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "          X : two-dimensional numpy array (n, m)\n",
        "          y : one-dimensional numpy array (n, )\n",
        "          w_init : initial values of weights. numpy array (m, )\n",
        "\n",
        "          L : learning rate (default 1e-2)\n",
        "          tol : tolerance (default 1e-3)\n",
        "          max_iters: maximum number of epochs\n",
        "    \n",
        "    Returns:\n",
        "          one-dimensional numpy array : weights\n",
        "    \"\"\"\n",
        "    w = w_init.copy()  # initialize of weights vector\n",
        "\n",
        "    n = X.shape[0] # number of rows\n",
        "    X_full = np.insert(X, 0, np.array([1]*n), axis=1)\n",
        "\n",
        "    # linear regression\n",
        "    for i in range(max_iters): \n",
        "        step = L * (1/n) * X_full.T.dot(X_full.dot(w) - y)\n",
        "        if np.all(abs(step) <= tol):  # if delta(difference) is too smal, let's stop\n",
        "            break\n",
        "        w = w - step  # update weights\n",
        "    return w"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EN6gKFKfDcJh"
      },
      "source": [
        "Now we need some data to test our multivariable linear regression. Let's use this dataset: https://www.kaggle.com/aungpyaeap/fish-market"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dsNrQvoTDxMF",
        "outputId": "aa711e26-cf21-47cd-ef2f-ec671b07288c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ymn9COf_D7Nd",
        "outputId": "58ff4b0a-28dc-4f91-fbbb-c1688a9c9636",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        }
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('/content/drive/MyDrive/Rango/data/Fish.csv')\n",
        "df"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "    Species  Weight  Length1  Length2  Length3   Height   Width\n",
              "0     Bream   242.0     23.2     25.4     30.0  11.5200  4.0200\n",
              "1     Bream   290.0     24.0     26.3     31.2  12.4800  4.3056\n",
              "2     Bream   340.0     23.9     26.5     31.1  12.3778  4.6961\n",
              "3     Bream   363.0     26.3     29.0     33.5  12.7300  4.4555\n",
              "4     Bream   430.0     26.5     29.0     34.0  12.4440  5.1340\n",
              "..      ...     ...      ...      ...      ...      ...     ...\n",
              "154   Smelt    12.2     11.5     12.2     13.4   2.0904  1.3936\n",
              "155   Smelt    13.4     11.7     12.4     13.5   2.4300  1.2690\n",
              "156   Smelt    12.2     12.1     13.0     13.8   2.2770  1.2558\n",
              "157   Smelt    19.7     13.2     14.3     15.2   2.8728  2.0672\n",
              "158   Smelt    19.9     13.8     15.0     16.2   2.9322  1.8792\n",
              "\n",
              "[159 rows x 7 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-a138c677-df3b-4b23-baea-b9c58c69d035\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Species</th>\n",
              "      <th>Weight</th>\n",
              "      <th>Length1</th>\n",
              "      <th>Length2</th>\n",
              "      <th>Length3</th>\n",
              "      <th>Height</th>\n",
              "      <th>Width</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Bream</td>\n",
              "      <td>242.0</td>\n",
              "      <td>23.2</td>\n",
              "      <td>25.4</td>\n",
              "      <td>30.0</td>\n",
              "      <td>11.5200</td>\n",
              "      <td>4.0200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Bream</td>\n",
              "      <td>290.0</td>\n",
              "      <td>24.0</td>\n",
              "      <td>26.3</td>\n",
              "      <td>31.2</td>\n",
              "      <td>12.4800</td>\n",
              "      <td>4.3056</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Bream</td>\n",
              "      <td>340.0</td>\n",
              "      <td>23.9</td>\n",
              "      <td>26.5</td>\n",
              "      <td>31.1</td>\n",
              "      <td>12.3778</td>\n",
              "      <td>4.6961</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Bream</td>\n",
              "      <td>363.0</td>\n",
              "      <td>26.3</td>\n",
              "      <td>29.0</td>\n",
              "      <td>33.5</td>\n",
              "      <td>12.7300</td>\n",
              "      <td>4.4555</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Bream</td>\n",
              "      <td>430.0</td>\n",
              "      <td>26.5</td>\n",
              "      <td>29.0</td>\n",
              "      <td>34.0</td>\n",
              "      <td>12.4440</td>\n",
              "      <td>5.1340</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>154</th>\n",
              "      <td>Smelt</td>\n",
              "      <td>12.2</td>\n",
              "      <td>11.5</td>\n",
              "      <td>12.2</td>\n",
              "      <td>13.4</td>\n",
              "      <td>2.0904</td>\n",
              "      <td>1.3936</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>155</th>\n",
              "      <td>Smelt</td>\n",
              "      <td>13.4</td>\n",
              "      <td>11.7</td>\n",
              "      <td>12.4</td>\n",
              "      <td>13.5</td>\n",
              "      <td>2.4300</td>\n",
              "      <td>1.2690</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>156</th>\n",
              "      <td>Smelt</td>\n",
              "      <td>12.2</td>\n",
              "      <td>12.1</td>\n",
              "      <td>13.0</td>\n",
              "      <td>13.8</td>\n",
              "      <td>2.2770</td>\n",
              "      <td>1.2558</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>157</th>\n",
              "      <td>Smelt</td>\n",
              "      <td>19.7</td>\n",
              "      <td>13.2</td>\n",
              "      <td>14.3</td>\n",
              "      <td>15.2</td>\n",
              "      <td>2.8728</td>\n",
              "      <td>2.0672</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>158</th>\n",
              "      <td>Smelt</td>\n",
              "      <td>19.9</td>\n",
              "      <td>13.8</td>\n",
              "      <td>15.0</td>\n",
              "      <td>16.2</td>\n",
              "      <td>2.9322</td>\n",
              "      <td>1.8792</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>159 rows Ã— 7 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a138c677-df3b-4b23-baea-b9c58c69d035')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-a138c677-df3b-4b23-baea-b9c58c69d035 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-a138c677-df3b-4b23-baea-b9c58c69d035');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6u9AoLtSFaHq"
      },
      "source": [
        "We have 7 features. `Species` is categorical column so we need to make it numeric to be able to use it in regression model. Usually we would use one hot encoding but for now let's just delete this column. `Weight` is our target value, let's look at correlation matrix:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XMl-J2OXGcmV",
        "outputId": "cba54bb4-87d4-4969-c5a9-33cd30448c97",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "source": [
        "df.corr()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "           Weight   Length1   Length2   Length3    Height     Width\n",
              "Weight   1.000000  0.915712  0.918618  0.923044  0.724345  0.886507\n",
              "Length1  0.915712  1.000000  0.999517  0.992031  0.625378  0.867050\n",
              "Length2  0.918618  0.999517  1.000000  0.994103  0.640441  0.873547\n",
              "Length3  0.923044  0.992031  0.994103  1.000000  0.703409  0.878520\n",
              "Height   0.724345  0.625378  0.640441  0.703409  1.000000  0.792881\n",
              "Width    0.886507  0.867050  0.873547  0.878520  0.792881  1.000000"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-2fdf9f05-cf82-422c-a538-af1a09cba379\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Weight</th>\n",
              "      <th>Length1</th>\n",
              "      <th>Length2</th>\n",
              "      <th>Length3</th>\n",
              "      <th>Height</th>\n",
              "      <th>Width</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Weight</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.915712</td>\n",
              "      <td>0.918618</td>\n",
              "      <td>0.923044</td>\n",
              "      <td>0.724345</td>\n",
              "      <td>0.886507</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Length1</th>\n",
              "      <td>0.915712</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.999517</td>\n",
              "      <td>0.992031</td>\n",
              "      <td>0.625378</td>\n",
              "      <td>0.867050</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Length2</th>\n",
              "      <td>0.918618</td>\n",
              "      <td>0.999517</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.994103</td>\n",
              "      <td>0.640441</td>\n",
              "      <td>0.873547</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Length3</th>\n",
              "      <td>0.923044</td>\n",
              "      <td>0.992031</td>\n",
              "      <td>0.994103</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.703409</td>\n",
              "      <td>0.878520</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Height</th>\n",
              "      <td>0.724345</td>\n",
              "      <td>0.625378</td>\n",
              "      <td>0.640441</td>\n",
              "      <td>0.703409</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.792881</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Width</th>\n",
              "      <td>0.886507</td>\n",
              "      <td>0.867050</td>\n",
              "      <td>0.873547</td>\n",
              "      <td>0.878520</td>\n",
              "      <td>0.792881</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2fdf9f05-cf82-422c-a538-af1a09cba379')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-2fdf9f05-cf82-422c-a538-af1a09cba379 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-2fdf9f05-cf82-422c-a538-af1a09cba379');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PH7Hsjy-HDkd"
      },
      "source": [
        "We can delete `Length2` as its correlation coefficient to `Length1` is 0.9995"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lu3zAUZZHb87",
        "outputId": "9c47f4c0-4906-4a63-bcbe-81f581087eb8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "y = np.array(df['Weight'])\n",
        "X = np.array(df.drop(columns=['Species', 'Weight', 'Length2']))\n",
        "\n",
        "X.shape, y.shape"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((159, 4), (159,))"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D28SvBXyGMv_",
        "outputId": "6cc278a7-c049-4750-a2a9-05870df036d4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler     \n",
        "\n",
        "# standartization of X\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "\n",
        "w_init = np.array([0]*5)  # initial weights\n",
        "w = vectorized_gradient_descent(X, y, w_init)\n",
        "w"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([398.32641509, 175.3714269 ,  49.31050861,  48.60814565,\n",
              "        82.49996959])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e5lZ599fL_g8"
      },
      "source": [
        "Let's look at $R^2$ (coefficient of determination) coefficient that will help us to estimate how good is our model.\n",
        "$$ \n",
        "R^2 = \n",
        "1 - \\frac{\\sum_{i=1}^{n} (y_i - y_i^\\ast)^2}{\\sum_{i=1}^{n} (y_i - \\bar{y})^2} \n",
        "$$\n",
        "\n",
        "$\\bar{y_i} = \\frac{1}{n} \\sum_{i=1}^{n} y_i$, $1 -$ mean square error / dispersion.   \n",
        "$R^2$ tells us what portion of target column variation was evaluated correctly by our model with its independent variables. We have this metric in sklearn library (metrics module) and model is as good as this score is near to 1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2SN6oTMbOoDn",
        "outputId": "e37c7364-0dd8-44cc-fa2b-e8ae4dd81c7d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from sklearn.metrics import r2_score\n",
        "\n",
        "X_full = np.insert(X, 0, np.array([1]*X.shape[0]), axis=1)\n",
        "y_pred = X_full.dot(w)\n",
        "\n",
        "r2_score(y, y_pred) "
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8817244431417081"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p85lNk2C_-Uk"
      },
      "source": [
        "## Stochastic Gradient Descent (SGD)  \n",
        "When we have too many rows, even vectorization isn't a good option because gradient descent(sometimes called as batch gradient descent) uses each row for each iteration. So here we have similar but different algorithms - stochastic gradient descent. It uses one row for each epoch, so it calculates result for each row than calculates error and updates weights. If we want to get a good result, we should go through each row for several times.\n",
        "Usually people don't use batch gradient descent and in sklearn we even don't have its implementation but we have SGD there."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DeBWfkLsVPU0",
        "outputId": "de347bf7-6619-4515-9a68-798160675aef",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from sklearn.linear_model import SGDRegressor\n",
        "\n",
        "reg = SGDRegressor(verbose=1)  # if verbose is one then there will be printed some informatino after each epoch\n",
        "reg.fit(X, y)\n",
        "\n",
        "print(reg.coef_) \n",
        "print(reg.intercept_)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-- Epoch 1\n",
            "Norm: 165.19, NNZs: 4, Bias: 186.778345, T: 159, Avg. loss: 59562.098086\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 179.73, NNZs: 4, Bias: 257.446651, T: 318, Avg. loss: 23539.634177\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 185.24, NNZs: 4, Bias: 300.071067, T: 477, Avg. loss: 14991.693555\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 187.30, NNZs: 4, Bias: 328.052666, T: 636, Avg. loss: 11370.141143\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 187.89, NNZs: 4, Bias: 346.754003, T: 795, Avg. loss: 9659.358134\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 185.32, NNZs: 4, Bias: 359.663913, T: 954, Avg. loss: 8793.389748\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 186.12, NNZs: 4, Bias: 369.344394, T: 1113, Avg. loss: 8335.684036\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 186.85, NNZs: 4, Bias: 376.161916, T: 1272, Avg. loss: 8074.669130\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 185.09, NNZs: 4, Bias: 381.201759, T: 1431, Avg. loss: 7924.638794\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 187.12, NNZs: 4, Bias: 385.114108, T: 1590, Avg. loss: 7838.698203\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 185.55, NNZs: 4, Bias: 387.823296, T: 1749, Avg. loss: 7784.941483\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 186.61, NNZs: 4, Bias: 389.957918, T: 1908, Avg. loss: 7769.484983\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 186.93, NNZs: 4, Bias: 392.152461, T: 2067, Avg. loss: 7748.348117\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 14\n",
            "Norm: 185.15, NNZs: 4, Bias: 393.374451, T: 2226, Avg. loss: 7712.248038\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 15\n",
            "Norm: 185.96, NNZs: 4, Bias: 394.473331, T: 2385, Avg. loss: 7726.848182\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 16\n",
            "Norm: 187.34, NNZs: 4, Bias: 395.558546, T: 2544, Avg. loss: 7710.151355\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 17\n",
            "Norm: 188.42, NNZs: 4, Bias: 396.071656, T: 2703, Avg. loss: 7700.734919\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 18\n",
            "Norm: 187.86, NNZs: 4, Bias: 396.626290, T: 2862, Avg. loss: 7710.531667\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 19\n",
            "Norm: 187.75, NNZs: 4, Bias: 396.941799, T: 3021, Avg. loss: 7702.435371\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 20\n",
            "Norm: 186.52, NNZs: 4, Bias: 397.237946, T: 3180, Avg. loss: 7694.974530\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 21\n",
            "Norm: 186.41, NNZs: 4, Bias: 397.562739, T: 3339, Avg. loss: 7700.547639\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 22\n",
            "Norm: 185.78, NNZs: 4, Bias: 397.682068, T: 3498, Avg. loss: 7694.231817\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 23\n",
            "Norm: 186.09, NNZs: 4, Bias: 397.860235, T: 3657, Avg. loss: 7698.897023\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 24\n",
            "Norm: 186.51, NNZs: 4, Bias: 398.031030, T: 3816, Avg. loss: 7695.163764\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 25\n",
            "Norm: 187.76, NNZs: 4, Bias: 398.045772, T: 3975, Avg. loss: 7686.587167\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 26\n",
            "Norm: 186.31, NNZs: 4, Bias: 398.246683, T: 4134, Avg. loss: 7681.131569\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 27\n",
            "Norm: 186.66, NNZs: 4, Bias: 398.277204, T: 4293, Avg. loss: 7686.843174\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 28\n",
            "Norm: 186.67, NNZs: 4, Bias: 398.234369, T: 4452, Avg. loss: 7689.552522\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 29\n",
            "Norm: 187.01, NNZs: 4, Bias: 398.275076, T: 4611, Avg. loss: 7689.488018\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 30\n",
            "Norm: 187.54, NNZs: 4, Bias: 398.286018, T: 4770, Avg. loss: 7679.517475\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 31\n",
            "Norm: 186.87, NNZs: 4, Bias: 398.253196, T: 4929, Avg. loss: 7684.951890\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 32\n",
            "Norm: 187.08, NNZs: 4, Bias: 398.232432, T: 5088, Avg. loss: 7687.112678\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 33\n",
            "Norm: 187.29, NNZs: 4, Bias: 398.284029, T: 5247, Avg. loss: 7686.594493\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 34\n",
            "Norm: 187.05, NNZs: 4, Bias: 398.332683, T: 5406, Avg. loss: 7683.809478\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 35\n",
            "Norm: 186.74, NNZs: 4, Bias: 398.315907, T: 5565, Avg. loss: 7683.508521\n",
            "Total training time: 0.01 seconds.\n",
            "Convergence after 35 epochs took 0.01 seconds\n",
            "[121.70912623 107.20042447  40.73824501  83.10063236]\n",
            "[398.31590659]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qoJPLrXCVbF7",
        "outputId": "3d0fce31-52ff-4f6f-f717-af6efd179ef0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "y_pred = reg.predict(X)\n",
        "\n",
        "r2_score(y, y_pred)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8806157976490949"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xAdBmD0hWOhJ"
      },
      "source": [
        "It's a little worse result than batch gradient descent but it's much faster."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6DaMsGJOkiFO"
      },
      "source": [
        "## Normal Equation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l7fQ0Taog1Zg"
      },
      "source": [
        "There is some analytical way to find a minimum - normal equation. We get normal equation by equaling zero to all partial derivatives of cost function because as we know from calculus this is the way to find minimum\n",
        "\n",
        "$$ \\frac{\\partial J(w) }{\\partial w} = \\frac{1}{n}X^T(Xw-y) = 0 $$\n",
        "\n",
        "multiply by $n$:\n",
        "$$ X^T(Xw-y) = 0 $$\n",
        "\n",
        "open brackets: \n",
        "$$ X^TXw - X^Ty = 0 $$\n",
        "\n",
        "$$ X^TXw = X^Ty $$\n",
        "\n",
        "multiply by $(X^TX)^{-1}$ from the left\n",
        "$$ w = (X^TX)^{-1}X^Ty $$\n",
        "\n",
        "We have normal equation. So we solve linear regression with only one equation but sometimes $X^TX$ is not invertable so we need to use pseudo inverse (`numpy.linalg.pinv()`). If we have all independent variables than this case should happen only if number of columns is more than number of rows and this is pretty rare case.  \n",
        "        \n",
        "advatnages of normal equation:  \n",
        "* we don't need to choose learning rate\n",
        "* we don't need updating weights  \n",
        "* we don't need feature scaling\n",
        "\n",
        "disadvatnages of normal equation:\n",
        "* is $m$ is too big, than it's slow because calculating $(X^TX)^{-1}$-áƒ˜áƒ¡ is difficult operation\n",
        "\n",
        "So if $m$ is big (probably more than 1000), we should use gradient descent "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EtQTMBgwT5HX"
      },
      "source": [
        "If we want to solve linear regression using normal equation, we can use `LinearRegression()` class from sklearn.linear_model library. It has `fit()` method(we should pass $X$ and $y$). `intercept_` gives us free variable and `coef_` - other weights."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cxoXV3OKwPmt",
        "outputId": "8ae5b87b-8c91-4cdd-f1c5-8971f4c5920c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from sklearn import linear_model\n",
        "\n",
        "reg = linear_model.LinearRegression()\n",
        "reg.fit(X, y)\n",
        "\n",
        "print(reg.coef_) \n",
        "print(reg.intercept_)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 565.38848272 -348.7203526   121.57902204   36.09084694]\n",
            "398.32641509433967\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WoVdKBmW2QDz",
        "outputId": "c2cdae85-f177-4d75-b457-911991d9ccbe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "y_pred = reg.predict(X)\n",
        "\n",
        "r2_score(y, y_pred)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8852683890437998"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pp3Q-kn-Upag"
      },
      "source": [
        "This is slightly better than gradient descent because we stopped gradient descent somewhere near minimum and normal equation gives us exact answer."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "y3YIOa-vTIuN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}